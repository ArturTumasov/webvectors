id,ru,en
base1,WebVectors: семантические модели для русского языка,WebVectors: word embeddings for Russian online
base2,Показать/скрыть меню,Toggle Navigation
base3,WebVectors,WebVectors
base4,О&nbsp;проекте,About
base5,Калькулятор,Calculator
base6,Похожие слова,Similar words
base7,Модели,Models
base8,Обучить свою модель,Train your model
base9,Публикации,Publications
base10,Контакты,Contacts
base11,Команда WebVectors,WebVectors Team
base12,Лицензия Creative Commons,Creative Commons License
base13,Визуализации,Visualizations
base14,Университет Осло, University of Oslo
base15,Национальный Исследовательский Университет Высшая Школа Экономики,National Research University Higher School of Economics
base16,Switch language,Сменить язык
base17,Различные операции,Miscellaneous
base18,"Слова, выделенные <span style='color: green;'>зеленым</span>, являются высокочастотными (доля слова в корпусе выше  0.00001); слова, выделенные <span style='color: red;'>красным</span>, являются низкочастотными (доля слова в корпусе ниже 0.0000005).","Words in <span style='color: green;'>green</span> are top frequent (corpus ratio higher than 0.00001); words in <span style='color: red;'>red</span>, are low frequent (corpus ratio less than 0.0000005)."
calc1,Семантический калькулятор,Semantic Calculator
calc2,Алгебраические операции,Algebraic operations
calc3,"Введите в&nbsp;&laquo;<strong>положительную</strong>&raquo; и&nbsp;&laquo;<strong>отрицательную</strong>&raquo; формы не&nbsp;более 10&nbsp;слов через пробел. <i>WebVectors</i> сложит вектора положительных слов и&nbsp;вычтет из&nbsp;них отрицательные. Затем он&nbsp;выдаст слова, наиболее близкие к&nbsp;получившемуся вектору. Если вы&nbsp;оставите отрицательное поле пустым, <i>WebVectors</i> просто найдет центр лексического кластера, образованного положительными словами.","Enter not more than 10&nbsp;space-separated words into <strong>positive</strong> and <strong>negative</strong> forms. <i>WebVectors</i> will sum up&nbsp;vectors for the positive words and subtract vectors from the negative ones. Then it&nbsp;will output the word closest to&nbsp;the resulting vector. If&nbsp;you leave negative form empty, <i>WebVectors</i> will simply find the center of&nbsp;word cluster formed by&nbsp;your positive words."
calc4,"Вы&nbsp;можете вычислять отношения. Например, &laquo;<strong>найти слово&nbsp;D, связанное со&nbsp;словом&nbsp;C таким&nbsp;же образом, как слово&nbsp;A связано со&nbsp;словом B</strong>&raquo;. Таким образом можно определять семантические связи между понятиями. В&nbsp;форме ввода приведен пример: какое слово относится к&nbsp;слову <strong>&laquo;лондон&raquo;</strong>, так&nbsp;же, как <strong>&laquo;россия&raquo;</strong> относится к&nbsp;<strong>&laquo;москве&raquo;</strong>? Ответ&nbsp;&#8212; &laquo;<strong>великобритания</strong>&raquo;: Лондон столица Великобритании, а&nbsp;Москва&nbsp;&#8212; столица России. <a onclick =""javascript:ShowHide('HiddenDiv')"" href=""javascript:;"" >Подробнее...</a></p>
<div class=""text"" id=""HiddenDiv"" style=""DISPLAY: none"" ><p><small>Можно сказать, что это вычитание компоненты &laquo;москва&raquo; из&nbsp;семантики слова &laquo;россия&raquo; и&nbsp;добавление компоненты &laquo;лондон&raquo;. Модель приходит к&nbsp;выводу, что &laquo;Россия для Лондона&nbsp;&#8212; это Великобритания&raquo; и&nbsp;выдает &laquo;Великобританию&raquo; в&nbsp;качестве ответа.</small></p>
<p><small>К&nbsp;словам можно дописывать символ подчеркивания &laquo;_&raquo; и&nbsp;<a href=""http://universaldependencies.org/u/pos/all.html"">тэг части речи</a> (<i>&laquo;река_NOUN&raquo;</i>). В&nbsp;ином случае, <i>WebVectors</i> определит часть речи автоматически.</small></p></div>","Calculate ratios, such as&nbsp;&laquo;<strong>find a&nbsp;word&nbsp;D related to&nbsp;the word&nbsp;C in&nbsp;the same way as&nbsp;the word&nbsp;A is&nbsp;related to&nbsp;the word B</strong>&raquo;. An&nbsp;example is&nbsp;given in&nbsp;the placeholder: which word is&nbsp;in&nbsp;the same relation to&nbsp;the word &laquo;<strong>Лондон</strong>&raquo; as&nbsp;&laquo;<strong>Россия</strong>&raquo; is&nbsp;to&nbsp;&laquo;<strong>Москва</strong>&raquo;? The answer is&nbsp;&laquo;<strong>Великобритания</strong>&raquo;: these concepts are in&nbsp;identical capital relations. <a onclick =""javascript:ShowHide('HiddenDiv')"" href=""javascript:;"" >More on this...</a></p>
<div class=""text"" id=""HiddenDiv"" style=""DISPLAY: none"" ><p><small>You can also think of&nbsp;this operation as&nbsp;removing the &laquo;Москва&raquo; component from the semantics of&nbsp;&laquo;Россия&raquo; and augmenting it&nbsp;with &laquo;Лондон&raquo; component. The model inferences that it&nbsp;should change the&nbsp;country. Thus, it&nbsp;outputs &laquo;Великобритания&raquo; as&nbsp;an&nbsp;answer.</small></p></div>"
calc5,"считает, что это будет:",thinks it&nbsp;will&nbsp;be:
calc7,НКРЯ,Ruscorpora
calc8,Русская Wikipedia,Russian Wikipedia
calc9,НКРЯ и&nbsp;русская Wikipedia,Ruscorpora and Russian Wikipedia
calc10,Веб-корпус,Web corpus
calc11,Новостной корпус,News corpus
calc13,телефон маленький,телефон маленький
calc18,Вычислить!,Calculate!
calc21,"Вы&nbsp;можете приписать к&nbsp;слову знак подчеркивания &laquo;_&raquo; и&nbsp;<a href=""http://universaldependencies.org/u/pos/all.html"">тэг части речи</a> (<i>&laquo;река_NOUN&raquo;</i>). В&nbsp;ином случае <i>WebVectors</i> определит часть речи самостоятельно.","You can optionally end the words with an&nbsp;underscore and a&nbsp;<a href=""http://universaldependencies.org/u/pos/all.html"">PoS tag</a> (<i>&laquo;река_NOUN&raquo;</i>). Otherwise, <i>WebVectors</i> will analyze words on&nbsp;its own."
calc23,НКРЯ,Ruscorpora
calc24,Русская Wikipedia,Russian Wikipedia
calc25,НКРЯ и&nbsp;русская Википедия,Ruscorpora and Russian Wikipedia
calc26,Веб-корпус,Web corpus
calc27,Новостной корпус,News corpus
calc31,Относится&nbsp;к,Relates&nbsp;to
calc32,"Вы&nbsp;также можете попробовать более сложные операции над векторами, чем простое решение пропорции.","If&nbsp;you feel confident with algebraic operations on&nbsp;vectors, you can try something more sophisticated than simple analogical inference."
calc33,москва,москва
calc34,россия,россия
calc35,лондон,лондон
description1,"РусВекторес: дистрибутивная семантика для русского языка, веб-интерфейс и модели для скачивания","WebVectors: word embeddings for Russian, web interface and models to download"
home1,WebVectors: семантические модели для русского языка,WebVectors: word embeddings for Russian online
home2,"Введите слово, чтобы получить список из&nbsp;10&nbsp;его ближайших семантических аналогов (квази-синонимов). <br/>Будет использована модель, обученная на русской Википедии и Национальном корпусе русского языка; другие модели вы можете найти на вкладке <a href=""similar/"">Похожие слова</a>.","Enter a&nbsp;word to&nbsp;produce a&nbsp;list of&nbsp;its 10&nbsp;nearest semantic associates.<br/>The model trained on Russian Wikipedia and Russian National Corpus will be&nbsp;used; for other models, visit <a href=""similar"">Similar Words</a> tab."
home3,Найти похожие слова!,Find similar words!
home4,Семантические аналоги для,Semantic associates for
home5,вычисленные на&nbsp;модели,computed&nbsp;on
home6,&#8217;You shall know a&nbsp;word by&nbsp;the company it&nbsp;keeps.&#8217; (Firth 1957).,&#8217;You shall know a&nbsp;word by&nbsp;the company it&nbsp;keeps.&#8217; (Firth 1957)
home7,мороз,мороз
home8,"16/12/2015&nbsp;&#8212; Обновлена <a href=""/models/"">модель на&nbsp;корпусе новостей</a>. Теперь она обучена на&nbsp;текстах вплоть до&nbsp;ноября 2015.","16/12/2015&nbsp;&#8212; <a href=""/en/models/"">News model</a> is&nbsp;updated. It&nbsp;is&nbsp;now trained on&nbsp;texts up&nbsp;to&nbsp;November 2015."
home9,"15/12/2015&nbsp;&#8212; В&nbsp;<a href=""/similar/"">&laquo;Похожие слова&raquo;</a> добавлен фильтр &laquo;Часть речи запроса&raquo;.","15/12/2015&nbsp;&#8212; In&nbsp;<a href=""/en/similar/"">Similar Words</a> one can now filter results with the query part of&nbsp;speech."
home10,"11/12/2015&nbsp;&#8212; Реализован API, отдающий ближайшие десять соседей для данных слова и&nbsp;модели. Результат можно получить в&nbsp;двух форматах: json и&nbsp;csv. Пример: <a href=""http://rusvectores.org/news/удар/api/csv/"">http://rusvectores.org/news/удар/api/csv/</a> или <a href=""http://rusvectores.org/news/удар/api/json/"">http://rusvectores.org/news/удар/api/json/</a>","11/12/2015&nbsp;&#8212; API is&nbsp;implemented. It&nbsp;outputs 10&nbsp;nearest neighbors for given word and model. There are two possible formats: json and csv. Example: <a href=""http://rusvectores.org/news/удар/api/json/"">http://rusvectores.org/news/удар/api/json/</a> or&nbsp;<a href=""http://rusvectores.org/news/удар/api/csv/"">http://rusvectores.org/news/удар/api/csv/</a>"
home11,"22/12/2015&nbsp;&#8212; Официально запущена версия <a href=""/ru/christmas/"">WebVectors 2.0: Christmas Edition</a>.","22/12/2015&nbsp;&#8212; <a href=""/en/christmas/"">WebVectors 2.0: Christmas Edition</a> is&nbsp;officially released."
home12,"03/02/2016&nbsp;&#8212; Исправлена ошибка, приводящая к&nbsp;невозможности <a href=""/ru/upload"">обучить собственную модель</a>.","03/02/2016&nbsp;&#8212; We&nbsp;fixed a&nbsp;bug because of&nbsp;which <a href=""/en/upload"">training user models</a> was broken."
home13,"15/03/2016&nbsp;&#8212; На&nbsp;движке WebVectors запущен <a href=""http://ltr.uio.no/semvec/"">веб-сервис с&nbsp;дистрибутивными моделями для английского и&nbsp;норвежского языков</a>.","15/03/2016&nbsp;&#8212; <a href=""http://ltr.uio.no/semvec/"">Web service with distributional models for English and Norwegian</a> is&nbsp;launched, based on&nbsp;WebVectors engine."
home14,"04/04/2016&nbsp;&#8212; Появилась возможность получать данные по&nbsp;API в&nbsp;формате json. Пример запроса&nbsp;&#8212; <a href=""http://rusvectores.org/news/праздник/api/json/"">http://rusvectores.org/news/праздник/api/json/</a>","04/04/2016&nbsp;&#8212; API now provides output in&nbsp;the json format. Example query&nbsp;&#8212; <a href=""http://rusvectores.org/news/удар/api/json/"">http://rusvectores.org/news/удар/api/json/</a>"
home15,"07/04/2016&nbsp;&#8212; Исходный код WebVectors полностью выложен на&nbsp;Github под названием <a href=""https://github.com/akutuzov/webvectors"">Webvectors</a>.","07/04/2016&nbsp;&#8212; WebVectors source code is&nbsp;released on&nbsp;Github as&nbsp;<a href=""https://github.com/akutuzov/webvectors"">Webvectors</a> framework."
home16,"01/07/2016&nbsp;&#8212; По&nbsp;соображениям безопасности, отключена возможность автоматически обучать модели на&nbsp;пользовательских корпусах. Тем не&nbsp;менее, если у&nbsp;вас есть интересный корпус, <a href=""http://rusvectores.org/ru/contacts/"">напишите нам</a>, и&nbsp;мы&nbsp;обязательно обучим для вас модель.","01/07/2016&nbsp;&#8212; For security reasons, the option to&nbsp;automatically train models on&nbsp;user-supplied corpora is&nbsp;now disabled. However, if&nbsp;you have an&nbsp;interesting corpus, <a href=""http://rusvectores.org/en/contacts/"">contact&nbsp;us</a>, and we&nbsp;will be&nbsp;glad to&nbsp;train a&nbsp;model for you."
home17,"22/10/2016&nbsp;&#8212; Появились подсказки при вводе запроса. NB: подсказки не&nbsp;полностью покрывают лексикон моделей. Если слово не&nbsp;появляется в&nbsp;подсказках, это не&nbsp;обязательно значит, что модели его не&nbsp;знают: возможно, оно просто редкое и&nbsp;странное.",22/10/2016&nbsp;&#8212; There are now hints as&nbsp;you type a&nbsp;query. Note that there are words not covered by&nbsp;hints as&nbsp;well (though they are rare and strange)!
home18,18/11/2016&nbsp;&#8212; API дополнен возможностью делать запросы о&nbsp;семантической схожести пар слов. Формат запроса: http://rusvectores.org/MODEL/WORD1__WORD2/api/similarity/,18/11/2016&nbsp;&#8212; API now allows to&nbsp;get semantic similarity of&nbsp;word pairs. Query format: http://rusvectores.org/MODEL/WORD1__WORD2/api/similarity/
home19,"02/02/2017&nbsp;&#8212; Существенно обновлены модели: новостной корпус теперь покрывает события вплоть до&nbsp;ноября 2016, дамп Википедии также обновлён до&nbsp;этой&nbsp;же даты. Кроме того, частеречные тэги переведены на&nbsp;стандарт <a href=""http://universaldependencies.org/u/pos/all.html"">Universal Tags</a>, а&nbsp;в&nbsp;словарях моделей появились двусловные словосочетания (биграммы).","02/02/2017&nbsp;&#8212; Major update of&nbsp;the models: news corpus now covers events up&nbsp;until November 2016, Wikipedia dump is&nbsp;updated to&nbsp;the same date. Additionally, PoS tags are converted to&nbsp;the <a href=""http://universaldependencies.org/u/pos/all.html"">Universal Parts of&nbsp;Speech</a> standard, and the models&#8217; vocabularies now feature multi-word-entities (bigrams)."
home20,"12/02/2017&nbsp;&#8212; Почитайте наш <a href=""/ru/rusvectores3/"">отчёт о&nbsp;проделанной работе за&nbsp;2016 год</a> и&nbsp;посмотрите <a href=""/ru/rusvectores3/"">новый скринкаст о&nbsp;работе с&nbsp;WebVectors</a>.","02/02/2017&nbsp;&#8212; Learn about <a href=""/en/rusvectores3/"">the new features we&nbsp;introduced in&nbsp;2016</a> and watch <a href=""/en/rusvectores3/"">new screencast video on&nbsp;working with WebVectors</a>."
home21,"09/03/2017&nbsp;&#8212; На&nbsp;<a href=""/ru/models"">отдельной странице с&nbsp;моделями</a> теперь можно скачать не&nbsp;только текущие модели, но&nbsp;и&nbsp;архивные, а&nbsp;также сравнить их&nbsp;друг с&nbsp;другом. Кроме того, мы&nbsp;добавили ссылки на&nbsp;русскоязычные тестовые сеты и&nbsp;на&nbsp;таблицу конверсии из&nbsp;тэгов Mystem в&nbsp;Universal PoS Tags.","09/03/2017&nbsp;&#8212; We&nbsp;present a&nbsp;<a href=""/en/models"">separate page for models</a> where you can download both recent and archived models, and compare them against each other. Also, we&nbsp;added the links to&nbsp;Russian test sets and to&nbsp;the conversion table from Mystem to&nbsp;the Universal PoS Tags."
home22,"30/06/2017&nbsp;&#8212; Существенно переработаны возможности <a href=""/ru/visual"">визуализации</a>. Теперь вы&nbsp;можете задавать несколько наборов слов: в&nbsp;визуализациях они будут раскрашены разными цветами. Если набор слов один, цвета будут соответствовать частям речи. Кроме того, одним кликом мыши можно визуализировать ваши данные в&nbsp;<a target=""_blank"" href=""http://projector.tensorflow.org/"">TensorFlow Embedding Projector</a>.","30/06/2017&nbsp;&#8212; <a href=""/en/visual"">Visualizations</a> are substantially upgraded. You can now use several word sets as&nbsp;an&nbsp;input: they will be&nbsp;labeled with different colors in&nbsp;the visualizations. If&nbsp;there is&nbsp;only one set, the colors will correspond to&nbsp;the words&#8217; parts of&nbsp;speech. Additionally, it&nbsp;is&nbsp;now possible to&nbsp;visualize your data in&nbsp;<a target=""_blank"" href=""http://projector.tensorflow.org/"">TensorFlow Embedding Projector</a> with one mouse click."
home23,"09/08/2017&nbsp;&#8212; Добавлена модель, обученная на&nbsp;одном из&nbsp;крупнейших русских веб-корпусов&nbsp;&#8212; <a href=""/ru/models/#araneum"">Araneum Russicum Maximum</a> (около 10&nbsp;миллиардов слов). Кроме того, все модели переоценены на&nbsp;более консистентном тестовом сете RuSimLex965.","30/06/2017&nbsp;&#8212; We&nbsp;added the model trained on&nbsp;<a href=""/en/models/#araneum"">Araneum Russicum Maximum</a>, which is&nbsp;one of&nbsp;the largest Russian web corpora (more than 10&nbsp;billion words). In&nbsp;addition, all the models are re-evaluated using RuSimLex965 semantic similarity test set (it&nbsp;is&nbsp;more consistent than RuSimLex999)."
home24,Новости проекта,Project news
similar0,Araneum fastText,Araneum fastText
similar1,Вычисление семантических ассоциатов,Computing associates
similar2,"Введите слово, чтобы получить список из&nbsp;10&nbsp;его ближайших семантических аналогов (квази-синонимов). Вы&nbsp;можете приписать к&nbsp;слову знак подчеркивания &laquo;_&raquo; и&nbsp;<a href=""http://universaldependencies.org/u/pos/all.html"" target=""_blank"">тэг части речи</a> (<i>&laquo;река_NOUN&raquo;</i>). Если вы&nbsp;этого не&nbsp;сделаете, <i>WebVectors</i> определит часть речи автоматически.","Enter a&nbsp;word to&nbsp;produce a&nbsp;list of&nbsp;its 10&nbsp;nearest semantic associates (quazy-synonyms). You can optionally end the word with an&nbsp;underscore and a&nbsp;<a href=""http://universaldependencies.org/u/pos/all.html"" target=""_blank"">PoS tag</a> (<i>&laquo;река_NOUN&raquo;</i>). Otherwise, <i>WebVectors</i> will detect&nbsp;it."
similar3,Выберите модель:,Choose the model:
similar4,НКРЯ,Ruscorpora
similar5,Английская Wikipedia,English Wikipedia
similar6,Британский Национальный Корпус,British National Corpus
similar7,Норвежский новостной корпус,Norsk Aviskorpus
similar8,Английский Gigaword,English Gigaword
similar9,Показывать только:,Show only:
similar10,Существительные,Nouns
similar11,Глаголы,Verbs
similar12,Наречия,Adverbs
similar13,Прилагательные,Adjectives
similar14,Все части речи,All of&nbsp;them
similar15,Найти похожие слова!,Find similar words!
similar16,Cемантические аналоги для,Semantic associates for
similar17,вычислено на&nbsp;модели,computed on&nbsp;data from
similar18,Модели неизвестно слово,The model does not know the word
similar19,Часть речи запроса,Query part of&nbsp;speech
similar20,Имена собственные,Proper names
similar21,Некорректный запрос,Incorrect query
similar22,Некорректный тэг,Incorrect tag
similar23,Нет подходящих результатов,No&nbsp;results
similar24,Вычисление семантической близости,Computing similarity
similar25,"Введите через пробел 2&nbsp;слова, чтобы вычислить их&nbsp;семантическое сходство. Можно также ввести несколько пар, разделяя их&nbsp;запятыми, как в&nbsp;примере.","Enter 2&nbsp;space-separated words to&nbsp;calculate their similarity. It&nbsp;is&nbsp;also possible to&nbsp;enter several pairs separating them with commas, as&nbsp;in&nbsp;the placeholder."
similar26,"movie film, city town","movie film, city town"
similar27,Вычислить семантическую близость!,Compute semantic similarity!
similar28,Пары слов,Word pairs
similar29,Косинусная близость,Cosine similarity
similar30,Нет близких слов с&nbsp;такой частью речи,No&nbsp;similar words with this tag
similar31,История запросов близости,Similarity queries history
similar32,Слова нет в словаре модели; вектор восстановлен из составляющих его символов.,The word is out of model vocabulary; its embedding is inferred from its characters.
synraw1,"Слова, семантически связанные&nbsp;с",Semantically related words for
synraw2,Какие слова близки к&nbsp;слову,What words are related&nbsp;to
synraw3,в,in&nbsp;the
synraw4,Показать вектор,Show the raw vector&nbsp;of
synraw5,в&nbsp;модели,in&nbsp;model
synraw6,Поискать,Search
synraw7,в&nbsp;Интернете,in&nbsp;the Internet
synraw8,на&nbsp;Wiktionary,in&nbsp;the Wiktionary
synraw9,О&nbsp;слове,About the word
synraw10,&#8212;&nbsp;визуализация вектора; по&nbsp;клику доступна полноразмерная версия,vector plot; click for full-size image
synraw11,в&nbsp;Википедии,in&nbsp;Wikipedia
synraw12,Это слово в&nbsp;других моделях,This word in&nbsp;other models
synraw13,в&nbsp;НКРЯ,in&nbsp;the RNC
synraw14,частота в корпусе,corpus frequency
synraw15,часть речи, part of speech
upload10,"Когда обучение закончится, вы&nbsp;сможете скачать вашу модель здесь:","When the training is&nbsp;finished, you can download your model here:"
upload11,Размерность векторов:,Vector size:
upload12,Алгоритм обучения:,Training mode:
upload13,Размер симметричного окна:,Symmetric window size:
upload1,Обучите свою модель,Train your model
upload2,Загрузите свой корпус!,Upload your own corpus!
upload3,"Введите URL, с&nbsp;которого можно загрузить ваш обучающий корпус. Корпус должен представлять собой txt-файл в&nbsp;кодировке UTF-8 (без некорректных символов), упакованный gzip, и&nbsp;содержащий одно предложение на&nbsp;каждой строке.",Enter a&nbsp;direct URL from where your training corpus can be&nbsp;downloaded. It&nbsp;should be&nbsp;a&nbsp;gzipped plain text UTF-8 (with no&nbsp;incorrect characters) document with one sentence per line.
upload4,Обработать,Process
upload5,Ваш файл,Your file from
upload6,отправлен в&nbsp;очередь на&nbsp;загрузку и&nbsp;обучение.,was put into downloading and training queue.
upload7,Ваш идентификатор обучения&nbsp;&#8212;,Your training identifier&nbsp;is
upload8,"Пожалуйста, сохраните его в&nbsp;надежном месте.","Please, write it&nbsp;down somewhere."
upload9,"Обучение вашей модели займет некоторое время. Типичная скорость обработки&nbsp;&#8212; около 50&nbsp;тысяч слов в&nbsp;секунду (иногда чуть медленнее, в&nbsp;зависимости от&nbsp;многих факторов).",It&nbsp;will take a&nbsp;while to&nbsp;train your model. General safe rule of&nbsp;thumb as&nbsp;for processing speed is&nbsp;approximately 50&nbsp;thousand words a&nbsp;second (plus some additional time for various system pipelines).
usermodel1,"Модели, созданные пользователями",User-generated models
usermodel2,Обучение модели на&nbsp;вашем корпусе завершено,Model training on&nbsp;your corpus has finished.
usermodel3,Здесь вы&nbsp;можете скачать вашу модель в&nbsp;бинарном формате Word2Vec (правая кнопка на&nbsp;ссылке&nbsp;&#8212; Скачать как):,Here you can download your model in&nbsp;binary Word2Vec format (right-click on&nbsp;the link below and press Save Link&nbsp;As):
usermodel4,Ваша модель с&nbsp;идентификатором,Your model with identifier
usermodel5,все еще обрабатывается. Приходите попозже.,is&nbsp;still being processed. Please come back later.
usermodel6,"Извините, идентификатор не&nbsp;распознан.","Unknown identifier, sorry."
visual1,Визуализация семантических связей между словами,Visualizing word inter-relations
visual2,Введите cлова через запятую. Мы&nbsp;построим карту их&nbsp;взаимного расположения в&nbsp;выбранной модели/моделях и&nbsp;отобразим двумерную проекцию этой карты (из&nbsp;векторного пространства высокой размерности).,"Enter a&nbsp;comma-separated list of&nbsp;words. We&nbsp;will build a&nbsp;map of&nbsp;their inter-relations in&nbsp;the chosen model(s), and return 2-dimensional version of&nbsp;this map (projected from high-dimensional vector space)."
visual3,Визуализация взаимного расположения слов при помощи t-SNE,Visual representation of&nbsp;word relations using t-SNE
visual4,Следующие слова неизвестны модели:,The following words were unknown to&nbsp;the model:
visual5,&#8212;&nbsp;это алгоритм снижения размерности и&nbsp;визуализации высокоразмерных данных. Он&nbsp;разработан Лоренсом ван дер Маатеном и&nbsp;описан в&nbsp;этой статье:,"is&nbsp;an&nbsp;algorithm for dimensionality reduction and visualization of&nbsp;high-dimensional datasets, developed by&nbsp;Laurens van der Maaten and described in&nbsp;this paper:"
visual6,Визуализация отношений между словами; по&nbsp;клику доступна полноразмерная версия,Plot for word relations; click for full-size image
visual7,"car,tank,transport,computer,mouse,Moscow,Paris,London,Russia,France,Britain,clean,fast,new","car,tank,transport,computer,mouse,Moscow,Paris,London,Russia,France,Britain,clean,fast,new"
visual8,Слишком мало слов,Too few words
visual9,Слова не&nbsp;должны повторяться,Words must be&nbsp;unique
visual15,Визуализировать,Visualize
visual16,Визуализация,Visualization
visual17,вычислено на&nbsp;модели,computed on&nbsp;data from
visual18,14&nbsp;тысяч наиболее частотных существительных в&nbsp;НКРЯ,14K most frequent nouns in&nbsp;the Russian National Corpus
visual19,Визуализация отношений между cуществительными; по&nbsp;клику доступна полноразмерная версия,Plot for noun relations; click for full-size image
visual20,Примеры больших семантических карт,Examples of&nbsp;large semantic maps
visual21,8&nbsp;тысяч наиболее частотных существительных в&nbsp;новостном корпусе,8K&nbsp;most frequent nouns in&nbsp;the news corpus
visual22,Визуализировать в&nbsp;TensorFlow Projector,Visualize in&nbsp;TensorFlow Projector
visual23,Показать трёхмерную проекцию взаимного расположения выбранных слов в&nbsp;TensorFlow Projector от&nbsp;Google,Show 3D&nbsp;projection of&nbsp;the chosen words in&nbsp;Google&#8217;s TensorFlow Projector
visual24,"Вы&nbsp;можете добавлять новые группы слов кнопкой &#8217;+&#8217;; они отобразятся на&nbsp;визуализации разными цветами (если группа одна, цвета соответствуют частям речи). Оптимальное общее количество слов&nbsp;&#8212; от&nbsp;7&nbsp;до&nbsp;20.","You can add new groups of&nbsp;words with the &#8217;+&#8217; button. They will be&nbsp;visualized with different colors (if&nbsp;there is&nbsp;only 1&nbsp;group, the colors marks parts of&nbsp;speech). Optimal total number of&nbsp;words is&nbsp;from 7&nbsp;to&nbsp;20."
