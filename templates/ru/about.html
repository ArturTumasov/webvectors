{% extends "base.html" %}

{% block content %}
        <h1>О сервисе</h1>
<p>Этот сервис вычисляет семантическое сходство между словами русского языка (определяет степень их "квазисинонимии"). Он назван по аналогии с <a href="http://ruscorpora.ru/ru">RusCorpora</a>, веб-сайтом Национального Корпуса Русского Языка (НКРЯ). На <i>RusCorpora</i> можно работать с корпусами (лат. <i>corpora</i>), а на нашем ресурсе - с лексическими векторами (лат. <i>vectōrēs</i>).</p>
<p>В последнее время существенно возрос интерес к <a href="https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D1%81%D1%82%D1%80%D0%B8%D0%B1%D1%83%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D0%BC%D0%B0%D0%BD%D1%82%D0%B8%D0%BA%D0%B0">дистрибутивной семантике</a>. В основном это обусловлено перспективностью подхода, использующего языковые модели на основе нейронных сетей, обученные на больших корпусах. При обучении мы получаем распределённые вектора для слов. Вероятно, самый известный на сегодня инструмент в этой области - <i><a href="https://code.google.com/p/word2vec/">word2vec</a></i>, позволяющий быстро получать вектора на огромных объёмах языкового материала.</p>
<blockquote><p><i>В дистрибутивной семантике слова обычно представляются в виде векторов в многомерном пространстве. Семантическое сходство вычисляется как простая <a href="https://en.wikipedia.org/wiki/Cosine_similarity">косинусная близость</a> между двумя векторами и может принимать значения в промежутке [0...1]. Значение <strong>0</strong> означает отсутствие похожих контекстов у этих слов, а <strong>1</strong> - напротив, свидетельствует о полной идентичности их контекстов.</i></p></blockquote>
<p><i>Word2vec</i> и подобные ему подходы подробно изучаются и тестируются в применении к английскому языку, в то время как для русского языка такая работа едва начата. Таким образом, важно предоставить русскоязычному лингвистическому сообществу доступ к соответствующим инструментам.</p>
<p>К сожалению, обучение и использование нейронных векторных моделей на основе больших корпусов может лежать за пределами возможностей домашних компьютеров и даже некоторых серверов. Вот почему мы создали этот сервис: мы хотим снизить порог входа для тех, кто хочет работать в этом новом и интересном направлении.</p>
<h2>Что такое <i>RusVectōrēs</i>?</h2>
<p><i>RusVectōrēs</i> - это семантический калькулятор, который работает с отношениями между словами в дистрибутивных моделях. Пользователь может выбрать одну или несколько моделей - в настоящий момент доступны <a href="/dsm/{{lang}}/models">4 модели</a>, обученные на разных корпусах (некоторые из них заняли ведущие места в <a href="http://russe.nlpub.ru/">соревновании RUSSE</a>). Каждая модель содержит от 200 тысяч до 1 миллиона лемм.</p>
<p>Выбрав модель, вы можете:</p>
<ol>
<li><a href="/dsm/{{lang}}/calculator">вычислять семантическое сходство</a> между парами слов;</li>
<li><a href="/dsm/{{lang}}/similar">находить слова, ближайшие к данному</a> (с возможностью фильтрации по части речи);</li>
<li><a href="/dsm/{{lang}}/similar">получать вектор (в виде массива чисел) данного слова в выбранной модели</a>;</li>
<li><a href="/dsm/{{lang}}/calculator">выполнять над векторами слов алгебраические операции </a> (сложение, вычитание, поиск центра лексического кластера и расстояний до этого центра).</li>
</ol>
<p>Последняя операция дает интересные результаты. Например, модель, обученная на русскоязычной Википедии и НКРЯ, возвращает слово <span style="color:green;"><i>быт</i></span> в результате вычитания слова <span style="color:green;"><i>смысл</i></span> из слова <span style="color:green;"><i>жизнь</i></span>. Это может показаться чем-то не слишком практичным, но исследования, уже проведенные для английского языка, доказали применимость таких отношений во многих областях, включая машинный перевод.</p>
<p>Естественно, можно сравнивать результаты разных моделей. Чтобы не ограничивать пользователей выбранными нами моделями, мы добавили возможность <a href="/dsm/{{lang}}/upload">загрузки своего корпуса</a>. Сервер обучит на нем модель <i>word2vec</i> с настройками, заданными пользователем.</p>
<blockquote><p><i>В духе парадигмы Semantic Web и Linked Data, каждое слово каждой модели обладает своим уникальным идентификатором URI, явно указывающим лемму, модель и часть речи (например, <a href="/dsm/{{lang}}/news/кризисный_A">http://ling.go.mail.ru/dsm/news/кризисный_A</a>). По запросу на этот адрес генерируется список десяти слов, ближайших к данной лемме в данной модели и принадлежащих к той же части речи, что и сама лемма.</i></p></blockquote>
<p>Нам бы хотелось, чтобы <i>RusVectōrēs</i> стал одним из узлов академической информации о нейронных языковых моделях для русского языка, поэтому на сайте имеется раздел <a href="/dsm/{{lang}}/publications">"Публикации"</a>, содержащий опубликованные научные работы и ссылки на другие полезные ресурсы. В то же время, мы надеемся, что <i>RusVectōrēs</i> популяризирует дистрибутивную семантику и компьютерную лингвистику и сделает их более доступными и привлекательными для русскоязычной публики.</p>
        <h1>Публикации</h1>
        <p class="lead">Если вам интересны дистрибутивные семантические модели, просмотрите следующие публикации:</p>
            <ol>
                <li>Bybee, J. <a href="http://www.citeulike.org/group/220/article/989323">Frequency of use and the organization of language</a>. (2006)</li>
                <li>Miller, George A., and Walter G. Charles. <a href="http://www.tandfonline.com/doi/abs/10.1080/01690969108406936">Contextual correlates of semantic similarity</a>. // in Language and cognitive processes 6.1 (1991): 1-28.</li>
                <li>Baroni, Marco, and Sabrina Bisi. <a href="http://clic.cimec.unitn.it/marco/publications/lrec2004/syn_lrec_2004.pdf">Using Cooccurrence Statistics and the Web to Discover Synonyms in a Technical Language</a>. // in LREC Proceedings. (2004)</li>
                <li>Turney, P. D., P. Pantel (2010). “<a href="http://www.aaai.org/Papers/JAIR/Vol37/JAIR-3705.pdf">From frequency to meaning: Vector space models of semantics</a>”. Journal of artificial intelligence research, 37(1), 141-188.</li>
                <li>Řehůřek, Radim, and Petr Sojka. <a href="http://radimrehurek.com/gensim/lrec2010_final.pdf">Software framework for topic modelling with large corpora</a>. // in Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks (2010).</li>
                <li>Panchenko A., et al.. "<a href="http://dial.academielouvain.be/vital/access/services/Download/boreal:120392/PDF_01">Serelex: Search and Visualization of Semantically Related Words</a>”. In Proceedings of the 35th European Conference on Information Retrieval (ECIR 2013). Springer's Lecture Notes on Computer Science, 2013, Moscow (Russia).</li>
                <li>Mikolov, T., et al. <a href="http://arxiv.org/abs/1301.3781">Efficient estimation of word representations in vector space</a>. arXiv preprint arXiv:1301.3781 (2013).</li>
                <li>Mikolov, Tomas, et al. “<a href="http://arxiv.org/abs/1309.4168">Exploiting similarities among languages for machine translation.</a>” arXiv preprint arXiv:1309.4168 (2013).</li>
                <li>Baroni, Marco, et al. "<a href="http://anthology.aclweb.org/P/P14/P14-1023.pdf">Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</a>” Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Vol. 1. (2014)</li>
                <li>Kutuzov, Andrey and Kuzmenko, Elizaveta. “<a href="https://www.academia.edu/11754162/Comparing_neural_lexical_models_of_a_classic_national_corpus_and_a_web_corpus_the_case_for_Russian">Comparing Neural Lexical Models of a Classic National Corpus and a Web Corpus: The Case for Russian</a>”.  A. Gelbukh (Ed.): CICLing 2015, Part I, Springer LNCS 9041, pp. 47–58, 2015. DOI: 10.1007/978-3-319-18111-0_4</li>
                <li>Bartunov Sergey et al. “<a href="http://arxiv.org/abs/1502.07257">Breaking Sticks and Ambiguities with Adaptive Skip-gram</a>”. Eprint arXiv:1502.07257, 02/2015</li>
                <li>O. Levy, Y. Goldberg, and I. Dagan “<a href="https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a>”. TACL 2015</li>
                <li>Xin Rong “<a href="http://arxiv.org/abs/1411.2738">word2vec Parameter Learning Explained</a>”. arXiv preprint arXiv:1411.2738 (2015)</li>
                <li>Kutuzov, Andrey and Andreev, Igor. “<a href="http://www.dialog-21.ru/digests/dialog2015/materials/pdf/KutuzovAAndreevI.pdf">Texts in, meaning out: neural language models in semantic similarity task for Russian.</a>” Proceedings of the Dialog 2015 Conference, Moscow, Russia (2015)</li>
                <li>Panchenko A., et al. "<a href="http://www.dialog-21.ru/digests/dialog2015/materials/pdf/PanchenkoAetal.pdf">RUSSE: The First Workshop on Russian Semantic Similarity</a>". Proceedings of the Dialogue 2015 conference, Moscow, Russia (2015)</li>
                <li>Arefyev N.V., et al. "<a href="http://www.dialog-21.ru/digests/dialog2015/materials/pdf/ArefyevNVetal.pdf">Evaluating three corpus-based semantic similarity systems for Russian</a>". Proceedings of the Dialogue 2015 conference, Moscow, Russia (2015)</li>
                <li>Lopukhin K.A., et al. "<a href="http://www.dialog-21.ru/digests/dialog2015/materials/pdf/LopukhinKAetal.pdf">The impact of different vector space models and supplementary techniques in Russian semantic similarity task</a>". Proceedings of the Dialogue 2015 conference, Moscow, Russia (2015)</li>
                <li><a href="https://www.youtube.com/watch?v=9JJsMWKcU7s" target="_blank">"Нейронные языковые модели и задача определения семантической близости слов"</a>, видеозапись лекции Андрея Кутузова на Moscow Data Science meetup 4 сентября 2015 (in Russian)
                </li>
                </ol>

        <h1>Модели</h1>
        <p>В настоящий момент доступны следующие модели:</p>
    <table class="table">
        <tr><td>
        * <strong><a name="ruscorpora">Национальный Корпус Русского языка</a></strong> (<i><a href="/static/models/ruscorpora.model.bin.gz">скачать модель</a>, 410 Мб</i>)
        <p>Обучена на полном <a href="http://ruscorpora.ru">НКРЯ</a>. Размер корпуса - <strong>107 561 399 токенов</strong>. Модель знает <strong>381 848 лемм</strong>; леммы, встретившиеся в корпусе один раз, при обучении игнорировались.</p>
        <p>Использован алгоритм Continuous Bag-of-Words.</p>
        <p>Размерность векторов 300, размер окна 2.</p>
        </td></tr>
        <tr><td>
        * <strong><a name="ruwikiruscorpora">НКРЯ и русскоязычная Wikipedia</a></strong> (<i><a href="/static/models/ruwikiruscorpora.model.bin.gz">скачать модель</a>, 1.1 Гб</i>)
        <p>Модель обучена на двух корпусах, слитых вместе. Размер корпуса - <strong>280 187 401 токенов</strong>. Модель знает <strong>604 043 леммы</strong>; леммы, встретившиеся в корпусе менее пяти раз, при обучении игнорировались.</p>
        <p>Использован алгоритм Continuous Bag-of-Words.</p>
        <p>Размерность векторов 500, размер окна 2.</p>
        </td></tr>
        <tr><td>
        * <strong><a name="web">Веб-корпус</a></strong> (<i><a href="/static/models/web.model.bin.gz">скачать модель</a>, 630 Мб</i>)
        <p>Модель обучена на случайном сэмпле из 9 миллионов русскоязычных веб-страниц. Размер корпуса - <strong>660 628 738 токенов</strong>. Модель знает <strong>353 608 лемм</strong>; леммы, встретившиеся в корпусе менее 30 раз, при обучении игнорировались.</p>
        <p>Использован алгоритм Continuous Skip-Gram.</p>
        <p>Размерность векторов 500, размер окна 2.</p>
        </td></tr>
        <tr><td>
        * <strong><a name="news">Новостной корпус</a></strong> (<i><a href="/static/models/news.model.bin.gz">скачать модель</a>, 594 Мб</i>)
        <p>Модель обучена на потоке новостей с 1500 российских новостных сайтов (от сентября 2013 до июня 2014, всего около 9 миллионов документов). Размер корпуса - <strong>1 289 145 897 токенов</strong>. Модель знает <strong>166 725 лемм</strong>; леммы, встретившиеся в корпусе менее 100 раз, при обучении игнорировались.</p>
        <p>Использован алгоритм Continuous Skip-Gram. Самый объёмный корпус, но и с наивысшим порогом частоты!</p>
        <p>Размерность векторов 1000, размер окна 20.</p>
        </td></tr>
        <tr><td>
        * <strong>Русскоязычная Wikipedia</strong> (в данный момент отключена по причине низкого качества модели)
        <p style="color:#888;">Обучена на всех страницах <a href="https://ru.wikipedia.org">русскоязычного раздела Wikipedia</a>. Размер корпуса - <strong>172 626 002 токенов</strong>. Модель знает <strong>1 016 238 лемм</strong>; леммы, встретившиеся в корпусе один раз, при обучении игнорировались.</p>
        <p style="color:#888;">Размерность векторов 300, размер окна 2.</p>
        </td></tr>
        </table>
        <i>Перед обучением все корпуса были токенизированы, разбиты на предложения, лемматизированы и размечены по частям речи. Стоп-слова (союзы, местоимения, предлоги, частицы и т.п.) были удалены. Модели, работающие с большими корпусами, намеренно обучались лишь на самых частотных словах, так как иначе модели были бы слишком тяжёлыми для веб-сервиса.</i>
{% endblock %}