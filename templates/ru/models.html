{% extends "base.html" %}

{% block content %}

        <h1>Модели</h1>
        <p>В настоящий момент доступны следующие модели:</p>
    <table class="table">
        <tr><td>
        * <strong><a name="ruscorpora">Национальный Корпус Русского языка</a></strong> (<i><a href="http://ling.go.mail.ru/static/models/ruscorpora.model.bin.gz">скачать модель</a>, 410 Мб</i>)
        <p>Обучена на полном <a href="http://ruscorpora.ru">НКРЯ</a>. Размер корпуса - <strong>107 561 399 токенов</strong>. Модель знает <strong>381 848 лемм</strong>; леммы, встретившиеся в корпусе один раз, при обучении игнорировались.</p>
        <p>Использован алгоритм Continuous Bag-of-Words.</p>
        <p>Размерность векторов 300, размер окна 2.</p>
        </td></tr>
        <tr><td>
        * <strong><a name="ruwikiruscorpora">НКРЯ и русскоязычная Wikipedia</a></strong>
        <p>Модель обучена на двух корпусах, слитых вместе. Размер корпуса - <strong>280 187 401 токенов</strong>. Модель знает <strong>604 043 леммы</strong>; леммы, встретившиеся в корпусе менее пяти раз, при обучении игнорировались.</p>
        <p>Использован алгоритм Continuous Bag-of-Words.</p>
        <p>Размерность векторов 500, размер окна 2.</p>
        </td></tr>
        <tr><td>
        * <strong><a name="web">Веб-корпус</a></strong> (<i><a href="http://ling.go.mail.ru/static/models/web.model.bin.gz">скачать модель</a>, 630 Мб</i>)
        <p>Модель обучена на случайном сэмпле из 9 миллионов русскоязычных веб-страниц. Размер корпуса - <strong>660 628 738 токенов</strong>. Модель знает <strong>353 608 лемм</strong>; леммы, встретившиеся в корпусе менее 30 раз, при обучении игнорировались.</p>
        <p>Использован алгоритм Continuous Skip-Gram.</p>
        <p>Размерность векторов 500, размер окна 2.</p>
        </td></tr>
        <tr><td>
        * <strong><a name="news">Новостной корпус</a></strong> (<i><a href="http://ling.go.mail.ru/static/models/news.model.bin.gz">скачать модель</a>, 594 Мб</i>)
        <p>Модель обучена на потоке новостей с 1500 российских новостных сайтов (от сентября 2013 до июня 2014, всего около 9 миллионов документов). Размер корпуса - <strong>1 289 145 897 токенов</strong>. Модель знает <strong>166 725 лемм</strong>; леммы, встретившиеся в корпусе менее 100 раз, при обучении игнорировались.</p>
        <p>Использован алгоритм Continuous Skip-Gram. Самый объёмный корпус, но и с наивысшим порогом частоты!</p>
        <p>Размерность векторов 1000, размер окна 20.</p>
        </td></tr>
        <tr><td>
        * <strong>Русскоязычная Wikipedia</strong> (в данный момент отключена по причине низкого качества модели)
        <p style="color:#888;">Обучена на всех страницах <a href="https://ru.wikipedia.org">русскоязычного раздела Wikipedia</a>. Размер корпуса - <strong>172 626 002 токенов</strong>. Модель знает <strong>1 016 238 лемм</strong>; леммы, встретившиеся в корпусе один раз, при обучении игнорировались.</p>
        <p style="color:#888;">Размерность векторов 300, размер окна 2.</p>
        </td></tr>
        </table>
        <i>Перед обучением все корпуса были токенизированы, разбиты на предложения, лемматизированы и размечены по частям речи. Стоп-слова (союзы, местоимения, предлоги, частицы и т.п.) были удалены. Модели, работающие с большими корпусами, намеренно обучались лишь на самых частотных словах, так как иначе модели были бы слишком тяжёлыми для веб-сервиса.</i>
{% endblock %}