{% extends "base.html" %}

{% block content %}

        <h1>Models</h1>
        <p>As of now, you can choose from the following models:</p>
    <table class="table">
        <tr><td>
        * <strong><a name="ruscorpora">Russian national corpus</a></strong> (<i><a href="http://ling.go.mail.ru/static/models/ruscorpora.model.bin.gz">download model</a>, 410 Mb</i>)
        <p>Trained on full <a href="http://ruscorpora.ru/en">Russian national corpus</a>. Corpus size is <strong>107 561 399 tokens</strong>. The model knows <strong>381 848 different lemmas</strong>; lemmas occurring only once were ignored.</p>
        <p>Trained using Continuous Bag-of-Words algorithm.</p>
        <p>Vector dimensionality was set to 300, window size 2.</p>
        </td></tr>
        <tr><td>
        * <strong><a name="ruwikiruscorpora">Russian national corpus and Russian Wikipedia together</a></strong>
        <p>Two corpora above were simply concatenated and the model trained on the resulting corpus. Corpus size is <strong>280 187 401 tokens</strong>. The model knows <strong>604 043 different lemmas</strong>; lemmas occurring less than 5 times were ignored.</p>
        <p>Trained using Continuous Bag-of-Words algorithm.</p>
        <p>Vector dimensionality was set to 500, window size 2.</p>
        </td></tr>
        <tr><td>
        * <strong><a name="web">Web corpus</a></strong> (<i><a href="http://ling.go.mail.ru/static/models/web.model.bin.gz">download model</a>, 630 MBytes</i>)
        <p>Trained on a collection of random Russian web pages, 9 million documents in total. Corpus size is <strong>660 628 738 tokens</strong>. The model knows <strong>353 608 different lemmas</strong>; lemmas occurring less than 30 times were ignored.</p>
        <p>Trained using Continuous Skip-Gram algorithm.</p>
        <p>Vector dimensionality was set to 500, window size 2.</p>
        </td></tr>
        <tr><td>
        * <strong><a name="news">News corpus</a></strong> (<i><a href="http://ling.go.mail.ru/static/models/news.model.bin.gz">download model</a>, 594 MBytes</i>)
        <p>Trained on news stream from 1500 Russian news sites (dated from September, 2013 to June, 2014, 9 million documents in total). Corpus size is <strong>1 289 145 897 tokens</strong>. The model knows <strong>166 725 different lemmas</strong>; lemmas occurring less than 100 times were ignored.</p>
        <p>Trained using Continuous Skip-Gram algorithm. Largest corpora, but with the highest frequency threshold!</p>
        <p>Vector dimensionality was set to 1000, window size 20.</p>
        </td></tr>
        <tr><td>
        * <strong>Russian Wikipedia</strong> (disabled as of now, because of poor quality)
        <p style="color:#888;">Trained on all pages of <a href="https://ru.wikipedia.org">Russian Wikipedia segment</a>. Corpus size is <strong>172 626 002 tokens</strong>. The model knows <strong>1 016 238 different lemmas</strong>; lemmas occurring only once were ignored.</p>
        <p style="color:#888;">Vector dimensionality was set to 300, window size 2.</p>
        </td></tr>
        </table>
        <i>Prior to training, all the corpora were tokenized, split into sentences, lemmatized and PoS-tagged. Stop words (conjunctions, pronouns, prepositions, particles, etc) were removed. For large corpora, models were intentionally trained on top-frequency words only, as otherwise model size would not be feasible for a web service.</i>
{% endblock %}