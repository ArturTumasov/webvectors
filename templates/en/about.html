{% extends "base.html" %}
{%- block title %}
     <title>RusVectōrēs: About</title>
 {%- endblock title %}



{% block content %}
        <h1>About</h1>
<p>This service computes semantic relations between words in Russian. It is named after <a href="http://ruscorpora.ru/en">RusCorpora</a>, the site for Russian National Corpus. They provide access to corpora, we provide access to semantic vectors (<i>vectōrēs</i> in Latin). These vectors reflect meaning based on word co-occurrence <strong>distribution</strong> in the training corpus (huge amounts of raw linguistic data).</p>
<blockquote><p>In <a href="https://en.wikipedia.org/wiki/Distributional_semantics">distributional semantics</a>, words are usually represented as vectors in a multi-dimensional space of their contexts. Semantic similarity between two words is then trivially calculated as a <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> between their corresponding vectors; it takes values between -1 and 1 (usually only values above 0 are used in practical tasks). <strong>0</strong> value roughly means the words lack similar contexts, and thus their meanings are unrelated to each other. <strong>1</strong> value means that the words' contexts are absolutely identical, and thus their meaning is very similar.</p></blockquote>
<p>Recently, distributional semantics received a substantially growing attention. The main reason for this is a very promising approach of employing artificial neural networks to learn hiqh-quality dense vectors (<strong>embeddings</strong>), using the so-called <strong>predictive models</strong>. The most well-known tool in this field now is possibly <i><a href="https://code.google.com/p/word2vec/">word2vec</a></i>, which allows very fast training, compared to previous approaches.</p>

<p><i>Word2vec</i>'s Skipgram and CBOW algorithms (and other similar methods) are being extensively studied and tested in application to English language. However, for Russian the surface is barely scratched. Thus, it is important to provide access to relevant tools and models for Russian linguistic community.</p>
<p>Unfortunately, training and querying neural embedding models for large corpora can be computationally expensive. Thus, we provide ready-made models trained on several Russian corpora, and a convenient web interface to query them. You can also download the models to process them on your own. Moreover, our web service features a bunch of (hopefully) useful visualizations for semantic relations between words. In general, the reason behind <i>RusVectōrēs</i> is to lower the entry threshold for those who want to work in this new and exciting field.</p>

<h2>What <i>RusVectōrēs</i> can do?</h2>
<p><i>RusVectōrēs</i> is basically a tool to explore relations between words in distributional models. You can think about it as a kind of <strong>`semantic calculator'</strong>. A user can choose one or several models to work with: currently we provide <a href="#models">four models</a> trained on different corpora (some of them have won top-ranking positions in the <a href="http://russe.nlpub.ru/">RUSSE evaluation track</a>). The models contain from 150K to 600K lemmas each.</p>
<p>After choosing a model, it is possible to:</p>
<ol>
<li><a href="/dsm/{{lang}}/similar">calculate semantic similarity</a> between pairs of words;</li>
<li><a href="/dsm/{{lang}}/similar">find words semantically closest to the query word</a> (optionally with part-of-speech filters);</li>
<li><a href="/dsm/{{lang}}/calculator">perform analogical inference</a>: find a word <strong>X</strong> which is related to the word <strong>Y</strong> in the same way as the word <strong>A</strong> is related to the word <strong>B</strong>;</li>
<li><a href="/dsm/{{lang}}/calculator">apply simple algebraic operations to word vectors</a> (addition, subtraction, finding average vector for a group of words and distances to this average value);</li>
<li><a href="/dsm/{{lang}}/visual">draw semantic maps</a> of relations between input words (it is useful to explore clusters and oppositions);</li>
<li><a href="/dsm/{{lang}}/similar">get the raw vectors (arrays of real values) and their visualizations for words in the chosen model</a>: just click on any word anywhere, or use a direct URI to the word of interest, as described below.</li>
</ol>
<blockquote><p>In the spirit of Semantic Web, each word in each model has its own unique URI explicitly stating lemma, model and part of speech (for example, <a href="/dsm/{{lang}}/news/кризисный_A">http://ling.go.mail.ru/dsm/en/news/кризисный_A</a>). Web pages at these URIs contain lists of the nearest semantic associates for the corresponding word, belonging to the same PoS as the word itself. Other information about the word is also shown.</p>
<p>We also provide a simple API to get the list of semantic associate for a given word in a given model. There are two possible formats: json and csv. Perform GET requests to URLs following the pattern <i>http://ling.go.mail.ru/dsm/MODEL/WORD/api/FORMAT</i> where MODEL is the <a href="#models">identifier</a> for the chosen model, WORD is the query word and FORMAT is "csv" or "json", depending on the output format you need. We will return a json file or a tab-separated text file with the first 10 associates.</p>
</blockquote>
<p>We recommend to experiment with algebraic operations on vectors, as they return interesting results. For example, the model trained on Russian Wikipedia and Russian National Corpus returns <span style="color:green;"><i>быт</i></span> if we subtract <span style="color:green;"><i>смысл</i></span> from <span style="color:green;"><i>жизнь</i></span>. This may sound like something not very practical, but the existing research in English models has already proved that such relationships can be useful for many applications including machine translation.</p>
<p>Naturally, one can compare results from different models on one screen. In order not to limit the users with the models of our choice, there is the possibility to <a href="/dsm/{{lang}}/upload">use one's own corpus</a>. The server will train a neural embedding model on it with user-defined settings.</p>

<p>We would like <i>RusVectōrēs</i> to become a hub of scholarly knowledge about neural semantic models for Russian, that's why there is a <a href="#publications">section with published academic papers and links to other relevant resources</a>. At the same time, we hope that <i>RusVectōrēs</i> will also popularize distributional semantics and computational linguistics, making it more understandable and attractive to the Russian-speaking public.</p>

        <h1><a name="publications"></a>Publications</h1>
        <p class="lead">If you are interested in distributional semantic models, you should really check these publications (in the chronological order):</p>
            <ol>
                <li>Bybee, J. <a href="http://www.citeulike.org/group/220/article/989323">Frequency of use and the organization of language</a>. (2006)</li>
                <li>Miller, G.A., and Walter, Ch. <a href="http://www.tandfonline.com/doi/abs/10.1080/01690969108406936">Contextual correlates of semantic similarity</a>. // in Language and cognitive processes 6.1 (1991): 1-28.</li>
                <li>Baroni, M., and S. Bisi. <a href="http://clic.cimec.unitn.it/marco/publications/lrec2004/syn_lrec_2004.pdf">Using Cooccurrence Statistics and the Web to Discover Synonyms in a Technical Language</a>. // in LREC Proceedings. (2004)</li>
                <li>Turney, P. D., P. Pantel (2010). “<a href="http://www.aaai.org/Papers/JAIR/Vol37/JAIR-3705.pdf">From frequency to meaning: Vector space models of semantics</a>”. Journal of artificial intelligence research, 37(1), 141-188.</li>
                <li>Řehůřek, R., and P. Sojka. <a href="http://radimrehurek.com/gensim/lrec2010_final.pdf">Software framework for topic modelling with large corpora</a>. // in Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks (2010).</li>
                <li>Panchenko A., et al.. "<a href="http://dial.academielouvain.be/vital/access/services/Download/boreal:120392/PDF_01">Serelex: Search and Visualization of Semantically Related Words</a>”. In Proceedings of the 35th European Conference on Information Retrieval (ECIR 2013). Springer's Lecture Notes on Computer Science, 2013, Moscow (Russia).</li>
                <li>Mikolov, T., et al. <a href="http://arxiv.org/abs/1301.3781">Efficient estimation of word representations in vector space</a>. arXiv preprint arXiv:1301.3781 (2013).</li>
                <li>Mikolov, T., et al. “<a href="http://arxiv.org/abs/1309.4168">Exploiting similarities among languages for machine translation.</a>” arXiv preprint arXiv:1309.4168 (2013).</li>
                <li>Baroni, M., et al. "<a href="http://anthology.aclweb.org/P/P14/P14-1023.pdf">Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</a>” Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Vol. 1. (2014)</li>
                <li>Kutuzov, A. and Kuzmenko, E. “<a href="https://www.academia.edu/11754162/Comparing_neural_lexical_models_of_a_classic_national_corpus_and_a_web_corpus_the_case_for_Russian">Comparing Neural Lexical Models of a Classic National Corpus and a Web Corpus: The Case for Russian</a>”.  A. Gelbukh (Ed.): CICLing 2015, Part I, Springer LNCS 9041, pp. 47–58, 2015. DOI: 10.1007/978-3-319-18111-0_4</li>
                <li>Bartunov S. et al. “<a href="http://arxiv.org/abs/1502.07257">Breaking Sticks and Ambiguities with Adaptive Skip-gram</a>”. Eprint arXiv:1502.07257, 02/2015</li>
                <li>O. Levy, Y. Goldberg, and I. Dagan “<a href="https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a>”. TACL 2015</li>
                <li>Xin Rong “<a href="http://arxiv.org/abs/1411.2738">word2vec Parameter Learning Explained</a>”. arXiv preprint arXiv:1411.2738 (2015)</li>
                <li>Kutuzov, A. and Andreev, I. “<a href="http://www.dialog-21.ru/digests/dialog2015/materials/pdf/KutuzovAAndreevI.pdf">Texts in, meaning out: neural language models in semantic similarity task for Russian.</a>” Proceedings of the Dialog 2015 Conference, Moscow, Russia (2015)</li>
                <li>Panchenko A., et al. "<a href="http://www.dialog-21.ru/digests/dialog2015/materials/pdf/PanchenkoAetal.pdf">RUSSE: The First Workshop on Russian Semantic Similarity</a>". Proceedings of the Dialogue 2015 conference, Moscow, Russia (2015)</li>
                <li>Arefyev N.V., et al. "<a href="http://www.dialog-21.ru/digests/dialog2015/materials/pdf/ArefyevNVetal.pdf">Evaluating three corpus-based semantic similarity systems for Russian</a>". Proceedings of the Dialogue 2015 conference, Moscow, Russia (2015)</li>
                <li>Lopukhin K.A., et al. "<a href="http://www.dialog-21.ru/digests/dialog2015/materials/pdf/LopukhinKAetal.pdf">The impact of different vector space models and supplementary techniques in Russian semantic similarity task</a>". Proceedings of the Dialogue 2015 conference, Moscow, Russia (2015)</li>
                <li><a href="https://www.youtube.com/watch?v=9JJsMWKcU7s" target="_blank">"Neural embeddings and lexical semantic similarity task"</a>, Andrey Kutuzov's talk at Moscow Data Science meetup 4 September 2015 (in Russian).
                </li>
                </ol>

        <h1><a name="models"></a>Models</h1>
        <p>As of now, you can choose from the following models:</p>
    <table class="table">
        <tr><td>
        * <strong><a name="ruscorpora">Russian national corpus</a></strong> (<i><a href="/static/models/ruscorpora.model.bin.gz">download model</a>, 302 Mb</i>) <strong>ruscorpora</strong>
        <p>Trained on full <a href="http://ruscorpora.ru/en">Russian national corpus</a>. Corpus size is <strong>107 561 399 tokens</strong>. The model knows <strong>281 776 different lemmas</strong>; lemmas occurring less than 3 times were ignored.</p>
        <p>Trained using Continuous Bag-of-Words algorithm.</p>
        <p>Vector dimensionality was set to 300, window size 2.</p>
        </td></tr>
        <tr><td>
        * <strong><a name="ruwikiruscorpora">Russian national corpus and Russian Wikipedia together</a></strong> (<i><a href="/static/models/ruwikiruscorpora.model.bin.gz">download model</a>, 1.1 Gb</i>) <strong>ruwikiruscorpora</strong>
        <p>Two corpora above were simply concatenated and the model trained on the resulting corpus. Corpus size is <strong>280 187 401 tokens</strong>. The model knows <strong>604 043 different lemmas</strong>; lemmas occurring less than 5 times were ignored.</p>
        <p>Trained using Continuous Bag-of-Words algorithm.</p>
        <p>Vector dimensionality was set to 500, window size 2.</p>
        </td></tr>
        <tr><td>
        * <strong><a name="web">Web corpus</a></strong> (<i><a href="/static/models/web.model.bin.gz">download model</a>, 630 MBytes</i>) <strong>web</strong>
        <p>Trained on a collection of random Russian web pages crawled in December 2014, 9 million documents in total. Corpus size is <strong>660 628 738 tokens</strong>. The model knows <strong>353 608 different lemmas</strong>; lemmas occurring less than 30 times were ignored.</p>
        <p>Trained using Continuous Skip-Gram algorithm.</p>
        <p>Vector dimensionality was set to 500, window size 2.</p>
        </td></tr>
        <tr><td>
        * <strong><a name="news">News corpus</a></strong> (<i><a href="/static/models/news.model.bin.gz">download model</a>, 524 MBytes</i>) <strong>news</strong>
        <p>Trained on news stream from 1500 primarily Russian-language news sites (15 million documents in total, dated from September 2013 to October 2015). Corpus size is <strong>2 439 237 751 tokens</strong>. The model knows <strong>147 358 different lemmas</strong>; lemmas occurring less than 200 times were ignored.</p>
        <p>Trained using Continuous Skip-Gram algorithm. Largest corpora, but with the highest frequency threshold!</p>
        <p>Vector dimensionality was set to 1000, window size 20.</p>
        </td></tr>
        </table>
        <i>Prior to training, all the corpora were tokenized, split into sentences, lemmatized and PoS-tagged using <a href="https://tech.yandex.ru/mystem/">Mystem</a>. Stop words (conjunctions, pronouns, prepositions, particles, etc) were removed. For large corpora, models were intentionally trained on top-frequency words only, as otherwise model size would not be feasible for a web service.</i>
    <h1>Links</h1>
    You can also check <a href="http://ltr.uio.no/semvec">SemanticVectors</a>, a similar service for English and Norwegian.
{% endblock %}
